{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import trackpy as tp\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from trackpy.utils import validate_tuple\n",
    "import trackpy.preprocessing as tpp\n",
    "import torchvision\n",
    "import scipy.ndimage as nd\n",
    "import hatzakis_lab_tracking as hlt\n",
    "device = \"cuda\"\n",
    "cpu = lambda x: np.array(np.squeeze(x.detach().cpu()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\andre\\Documents\\tracking_optimizations\\emily_tracking\\work_in_progress\\torch_port.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/tracking_optimizations/emily_tracking/work_in_progress/torch_port.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39margs.pickle\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m argfile, \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mres.pickle\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m resfile, \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mintpic.pickle\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m picfile:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/tracking_optimizations/emily_tracking/work_in_progress/torch_port.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     args \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(argfile)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/tracking_optimizations/emily_tracking/work_in_progress/torch_port.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     res \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(resfile)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/tracking_optimizations/emily_tracking/work_in_progress/torch_port.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     pic \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(picfile)\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "with open(\"args.pickle\", \"rb\") as argfile, open(\"res.pickle\", \"rb\") as resfile, open(\"intpic.pickle\", \"rb\") as picfile:\n",
    "    args = pickle.load(argfile)\n",
    "    res = pickle.load(resfile)\n",
    "    pic = pickle.load(picfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start \"bandpass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# device = \"cpu\"\n",
    "# image, lshort, llong, threshold, truncate = args\n",
    "image = hlt.image_loader_video(r\"C:\\Users\\andre\\Documents\\tracking_optimizations\\emily_tracking\\sample_vids\\Experiment_Process_001_20220823.tif\")\n",
    "lshort, llong, threshold, truncate = 1, 13, 0, 4\n",
    "lshort = validate_tuple(lshort, image.ndim)\n",
    "llong = validate_tuple(llong, image.ndim)\n",
    "timage = torch.tensor(image.astype(float)).reshape(1, 1, *image.shape).to(device)\n",
    "padder = (*(np.array(llong) // 2),)*2\n",
    "padded_timage = torch.nn.functional.pad(timage, padder, \"replicate\").to(device)\n",
    "\n",
    "gauss_ker = [tp.masks.gaussian_kernel(sigma, truncate) for sigma in lshort]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[[2148., 2125., 2094.,  ..., 2113., 2104., 2158.],\n",
       "            [2139., 2144., 2146.,  ..., 2101., 2089., 2129.],\n",
       "            [2126., 2171., 2220.,  ..., 2090., 2081., 2107.],\n",
       "            ...,\n",
       "            [2155., 2160., 2159.,  ..., 2174., 2128., 2039.],\n",
       "            [2139., 2161., 2185.,  ..., 2153., 2097., 2044.],\n",
       "            [2108., 2168., 2243.,  ..., 2123., 2055., 2050.]],\n",
       " \n",
       "           [[2098., 2136., 2207.,  ..., 2068., 2168., 2286.],\n",
       "            [2118., 2145., 2201.,  ..., 2083., 2135., 2226.],\n",
       "            [2148., 2162., 2199.,  ..., 2103., 2089., 2139.],\n",
       "            ...,\n",
       "            [2075., 2071., 2070.,  ..., 2109., 2141., 2185.],\n",
       "            [2079., 2075., 2076.,  ..., 2110., 2134., 2156.],\n",
       "            [2098., 2091., 2090.,  ..., 2105., 2107., 2099.]],\n",
       " \n",
       "           [[2144., 2141., 2143.,  ..., 2291., 2271., 2164.],\n",
       "            [2140., 2139., 2142.,  ..., 2222., 2204., 2159.],\n",
       "            [2131., 2140., 2153.,  ..., 2132., 2121., 2168.],\n",
       "            ...,\n",
       "            [2143., 2147., 2152.,  ..., 2148., 2127., 2198.],\n",
       "            [2108., 2112., 2119.,  ..., 2185., 2170., 2207.],\n",
       "            [2096., 2105., 2117.,  ..., 2187., 2194., 2162.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[2296., 2223., 2128.,  ..., 2119., 2127., 2182.],\n",
       "            [2265., 2225., 2180.,  ..., 2115., 2133., 2199.],\n",
       "            [2225., 2234., 2262.,  ..., 2124., 2145., 2212.],\n",
       "            ...,\n",
       "            [2231., 2225., 2207.,  ..., 2130., 2132., 2109.],\n",
       "            [2180., 2153., 2114.,  ..., 2149., 2125., 2069.],\n",
       "            [2128., 2101., 2063.,  ..., 2178., 2139., 2060.]],\n",
       " \n",
       "           [[2153., 2215., 2289.,  ..., 2144., 2118., 2102.],\n",
       "            [2137., 2208., 2308.,  ..., 2255., 2222., 2139.],\n",
       "            [2160., 2220., 2322.,  ..., 2387., 2353., 2194.],\n",
       "            ...,\n",
       "            [2089., 2129., 2188.,  ..., 2334., 2252., 2053.],\n",
       "            [2060., 2103., 2161.,  ..., 2260., 2216., 2089.],\n",
       "            [2054., 2080., 2110.,  ..., 2059., 2107., 2165.]],\n",
       " \n",
       "           [[2112., 2143., 2191.,  ..., 2075., 2083., 2123.],\n",
       "            [2114., 2145., 2193.,  ..., 2101., 2089., 2100.],\n",
       "            [2137., 2162., 2202.,  ..., 2133., 2097., 2070.],\n",
       "            ...,\n",
       "            [2167., 2154., 2137.,  ..., 2137., 2146., 2121.],\n",
       "            [2141., 2130., 2116.,  ..., 2150., 2184., 2192.],\n",
       "            [2083., 2080., 2079.,  ..., 2233., 2287., 2304.]]]]],\n",
       "        device='cuda:0', dtype=torch.float64),\n",
       " tensor([[[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]]]], device='cuda:0',\n",
       "        dtype=torch.float64))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_timage[:] = 0\n",
    "timage, padded_timage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Smoothing size must be an odd integer. Round up.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\andre\\Documents\\tracking_optimizations\\emily_tracking\\work_in_progress\\torch_port.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/tracking_optimizations/emily_tracking/work_in_progress/torch_port.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# %%timeit\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/tracking_optimizations/emily_tracking/work_in_progress/torch_port.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m box \u001b[39m=\u001b[39m tpp\u001b[39m.\u001b[39;49mboxcar(image, llong)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/tracking_optimizations/emily_tracking/work_in_progress/torch_port.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfilter\u001b[39m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull(llong, \u001b[39m1\u001b[39m\u001b[39m/\u001b[39mnp\u001b[39m.\u001b[39mproduct(llong), dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdouble)\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m*\u001b[39mllong)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/tracking_optimizations/emily_tracking/work_in_progress/torch_port.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m meaned \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mconv2d(padded_timage, \u001b[39mfilter\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\tracking-script\\lib\\site-packages\\trackpy\\preprocessing.py:74\u001b[0m, in \u001b[0;36mboxcar\u001b[1;34m(image, size)\u001b[0m\n\u001b[0;32m     72\u001b[0m size \u001b[39m=\u001b[39m validate_tuple(size, image\u001b[39m.\u001b[39mndim)\n\u001b[0;32m     73\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall([x \u001b[39m&\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m size]):\n\u001b[1;32m---> 74\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSmoothing size must be an odd integer. Round up.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m result \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m axis, _size \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(size):\n",
      "\u001b[1;31mValueError\u001b[0m: Smoothing size must be an odd integer. Round up."
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "box = tpp.boxcar(image, llong)\n",
    "filter = torch.full(llong, 1/np.product(llong), dtype = torch.double).reshape(1, 1, *llong).to(device)\n",
    "meaned = torch.nn.functional.conv2d(padded_timage, filter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_coords.is_floating_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret 'torch.float64' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\andre\\Documents\\tracking_optimizations\\emily_tracking\\work_in_progress\\torch_port.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/tracking_optimizations/emily_tracking/work_in_progress/torch_port.ipynb#Y423sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m np\u001b[39m.\u001b[39;49missubdtype(final_coords\u001b[39m.\u001b[39;49mdtype, torch\u001b[39m.\u001b[39;49mfloat)\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\tracking-script\\lib\\site-packages\\numpy\\core\\numerictypes.py:418\u001b[0m, in \u001b[0;36missubdtype\u001b[1;34m(arg1, arg2)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \u001b[39mReturns True if first argument is a typecode lower/equal in type hierarchy.\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    415\u001b[0m \n\u001b[0;32m    416\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issubclass_(arg1, generic):\n\u001b[1;32m--> 418\u001b[0m     arg1 \u001b[39m=\u001b[39m dtype(arg1)\u001b[39m.\u001b[39mtype\n\u001b[0;32m    419\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issubclass_(arg2, generic):\n\u001b[0;32m    420\u001b[0m     arg2 \u001b[39m=\u001b[39m dtype(arg2)\u001b[39m.\u001b[39mtype\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot interpret 'torch.float64' as a data type"
     ]
    }
   ],
   "source": [
    "np.issubdtype(final_coords.dtype, torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lshort' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\andre\\Documents\\tracking_optimizations\\emily_tracking\\work_in_progress\\torch_port.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/tracking_optimizations/emily_tracking/work_in_progress/torch_port.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# %%timeit\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/tracking_optimizations/emily_tracking/work_in_progress/torch_port.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m low \u001b[39m=\u001b[39m tpp\u001b[39m.\u001b[39mlowpass(image, lshort, truncate)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/tracking_optimizations/emily_tracking/work_in_progress/torch_port.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m lwx \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(truncate \u001b[39m*\u001b[39m lshort[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m0.5\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/tracking_optimizations/emily_tracking/work_in_progress/torch_port.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39m-\u001b[39mlwx, lwx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lshort' is not defined"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "low = tpp.lowpass(image, lshort, truncate)\n",
    "\n",
    "lwx = int(truncate * lshort[0] + 0.5)\n",
    "x = np.arange(-lwx, lwx + 1)\n",
    "lwy = int(truncate * lshort[1] + 0.5)\n",
    "y = np.arange(-lwy, lwy + 1)\n",
    "x = x.reshape(-1, 1)\n",
    "y = y.reshape(1, -1)\n",
    "r2 = x**2 + y**2\n",
    "res = np.exp(r2/(-2*lshort[0]**2))\n",
    "res /= res.sum()\n",
    "filter = torch.tensor(res).reshape(1, 1, *res.shape).to(device)\n",
    "\n",
    "blurred = torch.nn.functional.conv2d(timage, filter, padding = \"same\")\n",
    "abs(cpu(blurred) - low).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'blurred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\andre\\Documents\\tracking_optimizations\\emily_tracking\\work_in_progress\\torch_port.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/tracking_optimizations/emily_tracking/work_in_progress/torch_port.ipynb#Y111sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m result \u001b[39m=\u001b[39m blurred \u001b[39m-\u001b[39m meaned\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/tracking_optimizations/emily_tracking/work_in_progress/torch_port.ipynb#Y111sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(result \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m threshold, result, \u001b[39m0\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/tracking_optimizations/emily_tracking/work_in_progress/torch_port.ipynb#Y111sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m (cpu(result) \u001b[39m-\u001b[39m tpp\u001b[39m.\u001b[39mbandpass(\u001b[39m*\u001b[39margs))\u001b[39m.\u001b[39m\u001b[39m__abs__\u001b[39m()\u001b[39m.\u001b[39msum()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'blurred' is not defined"
     ]
    }
   ],
   "source": [
    "result = blurred - meaned\n",
    "result = torch.where(result >= threshold, result, 0)\n",
    "(cpu(result) - tpp.bandpass(*args)).__abs__().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End bandpass\n",
    "## Start grey_dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"grey_args.pckl\", \"rb\") as argfile:\n",
    "    args = pickle.load(argfile)\n",
    "image, separation, percentile, margin, precise = args\n",
    "# percentile = 64\n",
    "ndim = image.ndim\n",
    "size = [int(2 * s / np.sqrt(ndim)) for s in separation]\n",
    "padding = (*(np.array(size) // 2).astype(int),)\n",
    "timage = torch.tensor(image, dtype = torch.float).reshape(*image.shape)\n",
    "threshold = tp.find.percentile_threshold(image, percentile)\n",
    "shape = np.array(image.shape)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 512, 512]), torch.Size([512, 512]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape, timage.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dilation = torch.nn.functional.max_pool2d(timage, size, stride = 1, padding = padding)\n",
    "old = nd.grey_dilation(image, size, mode = \"constant\")\n",
    "(cpu(dilation).astype(int) == old).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxima = (image == cpu(dilation)) & (image > threshold)\n",
    "# maxima = np.nonzero(maxima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_pos = np.stack(np.nonzero(maxima)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (768) must match the size of tensor b (512) at non-singleton dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\andre\\Documents\\tracking_optimizations\\emily_tracking\\work_in_progress\\torch_port.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/tracking_optimizations/emily_tracking/work_in_progress/torch_port.ipynb#Y163sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m candidates \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnonzero((timage \u001b[39m==\u001b[39;49m dilation) \u001b[39m&\u001b[39m (timage \u001b[39m>\u001b[39m threshold))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/tracking_optimizations/emily_tracking/work_in_progress/torch_port.ipynb#Y163sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pos \u001b[39m=\u001b[39m candidates[:, \u001b[39m2\u001b[39m:]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (768) must match the size of tensor b (512) at non-singleton dimension 4"
     ]
    }
   ],
   "source": [
    "candidates = torch.nonzero((timage == dilation) & (timage > threshold))\n",
    "pos = candidates[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_boundary = ((pos < torch.tensor(margin)) | (pos > (torch.tensor(shape) - torch.tensor(margin) -1))).any(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2031, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates[~at_boundary].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cpu(pos) == old_pos).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End dilation\n",
    "## Start refine_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"com_args.pckl\", \"rb\") as file:\n",
    "    args = pickle.load(file)\n",
    "    (raw_image, image, radius, coords, max_iterations,\n",
    "            shift_thresh, characterize, walkthrough) = args\n",
    "shift_thresh = 0.5\n",
    "\n",
    "if not np.issubdtype(coords.dtype, np.integer):\n",
    "    raise ValueError('The coords array should be of integer datatype')\n",
    "ndim = image.ndim\n",
    "isotropic = np.all(radius[1:] == radius[:-1])\n",
    "mask = tp.masks.binary_mask(radius, ndim).astype(np.uint8)\n",
    "\n",
    "# Declare arrays that we will fill iteratively through loop.\n",
    "N = coords.shape[0]\n",
    "final_coords = np.empty_like(coords, dtype=np.float64)\n",
    "mass = np.empty(N, dtype=np.float64)\n",
    "raw_mass = np.empty(N, dtype=np.float64)\n",
    "if characterize:\n",
    "    if isotropic:\n",
    "        Rg = np.empty(N, dtype=np.float64)\n",
    "    else:\n",
    "        Rg = np.empty((N, len(radius)), dtype=np.float64)\n",
    "    ecc = np.empty(N, dtype=np.float64)\n",
    "    signal = np.empty(N, dtype=np.float64)\n",
    "\n",
    "ogrid = np.ogrid[[slice(0, i) for i in mask.shape]]  # for center of mass\n",
    "ogrid = [g.astype(float) for g in ogrid]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -r 7 -n 1\n",
    "traw = torch.tensor(raw_image).reshape(1, 1, *image.shape).to(device)\n",
    "timage = torch.tensor(image, dtype = torch.float).reshape(1, 1, *image.shape).to(device)\n",
    "tradius = torch.tensor(radius).to(device)\n",
    "gridx = torch.tensor(ogrid[0]).to(device).reshape(1, -1, 1)\n",
    "gridy = torch.tensor(ogrid[1]).to(device).reshape(1, 1, -1)\n",
    "tmask = torch.tensor(mask).to(device)\n",
    "pos = torch.tensor(coords).to(device)\n",
    "frames = candidates[~at_boundary, 0].reshape(-1, 1, 1)\n",
    "\n",
    "final_coords = torch.full((pos.shape[0], 2), torch.tensor(float(\"nan\")).to(device), dtype = float).to(device)\n",
    "final_masses = torch.full((pos.shape[0],), torch.tensor(float(\"nan\")).to(device), dtype = torch.float32).to(device)\n",
    "final_signals = torch.full((pos.shape[0],), torch.tensor(float(\"nan\")).to(device), dtype = torch.float32).to(device)\n",
    "\n",
    "\n",
    "not_done = torch.arange(pos.shape[0])\n",
    "make_inds_for_dim = lambda dim: torch.arange(-radius[dim], radius[dim] + 1)[:, None]\n",
    "\n",
    "upper_bound = torch.tensor(image.shape).to(device) - 1 - tradius.to(device)\n",
    "grids = [torch.moveaxis(\n",
    "    torch.arange(-radius[dim], radius[dim] + 1)[:, None].to(device)\n",
    "     , 0, dim) \n",
    "    for dim in range(image.ndim)]\n",
    "make_inds_for_dim = lambda dim, pos: grids[dim] + pos[:, dim].reshape(-1, 1, 1)\n",
    "\n",
    "for i in range(max_iterations):\n",
    "    neighborhoods = timage[frames, 0, make_inds_for_dim(0, pos), make_inds_for_dim(1, pos)]\n",
    "    neighborhoods = torch.multiply(neighborhoods, tmask, out = neighborhoods)\n",
    "    masses = neighborhoods.sum(axis = (1, 2))\n",
    "    cm_x = (neighborhoods * gridx).sum(axis = (1, 2)) / masses\n",
    "    cm_y = (neighborhoods * gridy).sum(axis = (1, 2)) / masses\n",
    "    cm = torch.stack((cm_x, cm_y), axis = 1)\n",
    "    off_center = cm - torch.tensor(radius).to(device)\n",
    "    back_shift = off_center < -shift_thresh\n",
    "    forward_shift = off_center > shift_thresh\n",
    "    keep_iterating = (back_shift | forward_shift).any(axis = 1)\n",
    "    if (~keep_iterating).sum() != 0:    \n",
    "        newly_done = not_done[~keep_iterating]\n",
    "        \n",
    "        final_coords[newly_done] = cm[~keep_iterating] - tradius + pos[~keep_iterating]\n",
    "        final_masses[newly_done] = masses[~keep_iterating]\n",
    "        final_signals[newly_done] = neighborhoods[~keep_iterating].max()\n",
    "    \n",
    "\n",
    "    if keep_iterating.sum() == 0:\n",
    "        break\n",
    "    pos[back_shift] -= 1\n",
    "    pos[forward_shift] += 1\n",
    "    # # pass\n",
    "    pos = pos[keep_iterating, :].clip(tradius, upper_bound)\n",
    "    frames = frames[keep_iterating, :, :]\n",
    "    not_done = not_done[keep_iterating]\n",
    "\n",
    "if not keep_iterating.sum() == 0:\n",
    "    rest = not_done[keep_iterating]\n",
    "    final_coords[rest] = cm[keep_iterating] - tradius + pos[keep_iterating]\n",
    "    final_masses[rest] = masses[keep_iterating]\n",
    "    final_signals[rest] = neighborhoods[keep_iterating].max()\n",
    "\n",
    "frames = candidates[~at_boundary, 0].reshape(-1, 1, 1)\n",
    "final_pixels = torch.round(final_coords).to(int)\n",
    "raw_masses = (traw[frames, 0, make_inds_for_dim(0, final_pixels), make_inds_for_dim(1, final_pixels)] * tmask).sum(axis = (1,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"outs.pckl\", \"rb\") as file:\n",
    "    old_raw, old_final_coords, old_signal, old_mass = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cpu(final_coords) == old_final_coords).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cpu(final_masses) == old_mass).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.  , 1080.75,    0.  , ...,    0.  ,    0.  ,    0.  ])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_raw - cpu(raw_masses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"letmesee.csv\", old_raw - cpu(raw_masses), fmt = \"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat, coord in enumerate(coords):\n",
    "    for iteration in range(max_iterations):\n",
    "        # Define the circular neighborhood of (x, y).\n",
    "        rect = tuple([slice(c - r, c + r + 1)\n",
    "                        for c, r in zip(coord, radius)])\n",
    "        neighborhood = mask * image[rect]\n",
    "        cm_n = tp.refine.center_of_mass._safe_center_of_mass(neighborhood, radius, ogrid)\n",
    "        break\n",
    "        cm_i = cm_n - radius + coord  # image coords\n",
    "\n",
    "        off_center = cm_n - radius\n",
    "        # logger.debug('off_center: %f', off_center)\n",
    "        if np.all(np.abs(off_center) < shift_thresh):\n",
    "            break  # Accurate enough.\n",
    "        # If we're off by more than half a pixel in any direction, move..\n",
    "        coord[off_center > shift_thresh] += 1\n",
    "        coord[off_center < -shift_thresh] -= 1\n",
    "        # Don't move outside the image!\n",
    "        upper_bound = np.array(image.shape) - 1 - radius\n",
    "        coord = np.clip(coord, radius, upper_bound).astype(int)\n",
    "        break\n",
    "    if feat == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_tracking\n",
    "import torch\n",
    "import hatzakis_lab_tracking as hlt\n",
    "import importlib \n",
    "importlib.reload(torch_tracking)\n",
    "params = hlt.Params(\n",
    "    lip_int_size = 8,  #originally 15 for first attemps\n",
    "    lip_BG_size = 70,   # originally 40 for first attemps\n",
    "    gap_size = 2, # adding gap\n",
    "    \n",
    "    dynamic_sep = 7,   # afstand mellem centrum af to partikler, 7 er meget lidt så skal være højere ved lavere densitet\n",
    "    dynamic_mean_multiplier = 12,  #hvor mange partikler finder den, around 1-3, lavere giver flere partikler\n",
    "    dynamic_object_size = 11, #diameter used in tp.locate, odd integer\n",
    "    dynamic_search_range = 4,\n",
    "    dynamic_memory = 0,\n",
    "    \n",
    "    static_sep = 8,\n",
    "    static_mean_multiplier = 4,\n",
    "    static_object_size = 11,\n",
    ")\n",
    "tvid = torch.tensor(hlt.image_loader_video(r\"C:\\Users\\andre\\Documents\\tracking_optimizations\\emily_tracking\\sample_vids\\c_20.tif\").astype(float), device = \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "idk = (slice(None),)+(None,)*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[idk].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected stride to be a single integer value or a list of 3 values to match the convolution dimensions, but got stride=[1, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\andre\\Documents\\tracking_optimizations\\emily_tracking\\work_in_progress\\torch_port.ipynb Cell 35\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/tracking_optimizations/emily_tracking/work_in_progress/torch_port.ipynb#Y444sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m torch_tracking\u001b[39m.\u001b[39;49mlocate(tvid, params\u001b[39m.\u001b[39;49mstatic_object_size, separation \u001b[39m=\u001b[39;49m params\u001b[39m.\u001b[39;49mstatic_sep)\n",
      "File \u001b[1;32mc:\\Users\\andre\\Documents\\tracking_optimizations\\emily_tracking\\work_in_progress\\torch_tracking.py:62\u001b[0m, in \u001b[0;36mlocate\u001b[1;34m(raw_video, diameter, minmass, maxsize, separation, noise_size, smoothing_size, threshold, percentile, topn, preprocess, max_iterations, device)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39m# Determine `video`: the video to find the local maxima on.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39mif\u001b[39;00m preprocess:\n\u001b[1;32m---> 62\u001b[0m     video \u001b[39m=\u001b[39m bandpass(raw_video, noise_size, smoothing_size, threshold)\n\u001b[0;32m     63\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     video \u001b[39m=\u001b[39m raw_video\n",
      "File \u001b[1;32mc:\\Users\\andre\\Documents\\tracking_optimizations\\emily_tracking\\work_in_progress\\torch_tracking.py:157\u001b[0m, in \u001b[0;36mbandpass\u001b[1;34m(video, lshort, llong, threshold, truncate)\u001b[0m\n\u001b[0;32m    155\u001b[0m padder \u001b[39m=\u001b[39m (\u001b[39m*\u001b[39m(np\u001b[39m.\u001b[39marray(llong) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m),)\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m    156\u001b[0m padded_video \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mpad(video, padder, \u001b[39m\"\u001b[39m\u001b[39mreplicate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 157\u001b[0m background \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mconv2d(padded_video, \u001b[39mfilter\u001b[39;49m)\n\u001b[0;32m    158\u001b[0m \u001b[39mdel\u001b[39;00m padded_video\n\u001b[0;32m    159\u001b[0m \u001b[39mfilter\u001b[39m \u001b[39m=\u001b[39m make_gaussian_filter(lshort, truncate, device)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected stride to be a single integer value or a list of 3 values to match the convolution dimensions, but got stride=[1, 1]"
     ]
    }
   ],
   "source": [
    "torch_tracking.locate(tvid, params.static_object_size, separation = params.static_sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tracking-script')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3409497d58d6e5264400e5348aa6fd098ba78c0f60b1c52b60052994396a7143"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
